{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, average_precision_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# preds\n",
    "\n",
    "# Load the training set of meta-model\n",
    "lipo_chemberta2_valid2 = pd.read_csv('./chemberta2/results/lipo/chemberta2_valid2_lipo_3_predictions.csv')\n",
    "lipo_molformer_valid2 = pd.read_csv('./molformer/results/lipo/molformer_valid2_lipo_3.csv')\n",
    "lipo_molbert_valid2 = pd.read_csv('./molbert/results/lipo/molbert_valid2_lipo_3.csv')\n",
    "\n",
    "# Load the test data for each model\n",
    "lipo_chemberta2_test = pd.read_csv('./chemberta2/results/lipo/chemberta2_test_lipo_3_predictions.csv')\n",
    "lipo_molformer_test = pd.read_csv('./molformer/results/lipo/molformer_test_lipo_3.csv')\n",
    "lipo_molbert_test = pd.read_csv('./molbert/results/lipo/molbert_test_lipo_3.csv')\n",
    "\n",
    "train_mean = 2.1720992063492064\n",
    "train_sd = 1.201255528709618\n",
    "\n",
    "# features\n",
    "\n",
    "lipo_chemberta2_features_valid2 = pd.read_csv('./chemberta2/features/lipo/chemberta2_valid2_lipo_3_features.csv')\n",
    "lipo_chemberta2_features_test = pd.read_csv('./chemberta2/features/lipo/chemberta2_test_lipo_3_features.csv')\n",
    "\n",
    "lipo_molformer_features_valid2 = pd.read_csv('./molformer/features/lipo/molformer_valid2_lipo_3_features.csv')\n",
    "lipo_molformer_features_test = pd.read_csv('./molformer/features/lipo/molformer_test_lipo_3_features.csv')\n",
    "\n",
    "lipo_molbert_features_valid2 = pd.read_csv('./molbert/features/lipo/molbert_valid2_lipo_3_features.csv')\n",
    "lipo_molbert_features_test = pd.read_csv('./molbert/features/lipo/molbert_test_lipo_3_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Lipo (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the actual and predicted values\n",
    "\n",
    "train_mean = 2.1720992063492064\n",
    "train_sd = 1.201255528709618\n",
    "\n",
    "# Chemberta2\n",
    "lipo_chemberta_actual = lipo_chemberta2_test['target']\n",
    "lipo_chemberta_pred = lipo_chemberta2_test['pred_raw']\n",
    "\n",
    "# Molformer\n",
    "lipo_molformer_actual = lipo_molformer_test['target']\n",
    "lipo_molformer_pred = lipo_molformer_test['pred_raw']\n",
    "\n",
    "# molbert\n",
    "lipo_molbert_actual = lipo_molbert_test['target_raw']\n",
    "lipo_molbert_pred = lipo_molbert_test['pred_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chemberta2': {'MAE': 0.46657646298682925,\n",
       "  'RMSE': 0.6194209838770622,\n",
       "  'R2 Score': 0.7336677436091222,\n",
       "  'Correlation': 0.8681521305961759},\n",
       " 'Molformer': {'MAE': 0.4598245026857143,\n",
       "  'RMSE': 0.6138971242226391,\n",
       "  'R2 Score': 0.7383967476487712,\n",
       "  'Correlation': 0.8677855416135948},\n",
       " 'Molbert': {'MAE': 0.5155634538071429,\n",
       "  'RMSE': 0.6599043655043522,\n",
       "  'R2 Score': 0.697716846760879,\n",
       "  'Correlation': 0.8363791911807847}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating metrics\n",
    "lipo_metrics_results = {}\n",
    "\n",
    "for model_name, actual, pred in [(\"Chemberta2\", lipo_chemberta_actual, lipo_chemberta_pred),\n",
    "                                 (\"Molformer\", lipo_molformer_actual, lipo_molformer_pred),\n",
    "                                 (\"Molbert\", lipo_molbert_actual, lipo_molbert_pred)]:\n",
    "    lipo_metrics_results[model_name] = {\n",
    "        \"MAE\": mean_absolute_error(actual, pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(actual, pred)),\n",
    "        \"R2 Score\": r2_score(actual, pred),\n",
    "        \"Correlation\": pearsonr(actual, pred)[0]  # Only record the correlation coefficient\n",
    "    }\n",
    "\n",
    "lipo_metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized valid2 labels\n",
    "lipo_y_ensemble_valid2 = (lipo_chemberta2_valid2['target'] - train_mean)/train_sd\n",
    "\n",
    "# Create the features for the ensemble from the prediction probabilities of being in class 1\n",
    "lipo_X_ensemble_valid2 = pd.concat([\n",
    "    lipo_chemberta2_valid2['pred_z'] - lipo_y_ensemble_valid2,\n",
    "    lipo_molformer_valid2['pred_z'] - lipo_y_ensemble_valid2,\n",
    "    # add features from training set\n",
    "    lipo_chemberta2_features_valid2.iloc[:, 2:],\n",
    "    lipo_molformer_features_valid2.iloc[:, 1:],\n",
    "], axis=1)\n",
    "\n",
    "# change feature names of the ensemble so that they are unique\n",
    "lipo_X_ensemble_valid2.columns = ['chemberta', 'molformer'] + list(lipo_chemberta2_features_valid2.columns[2:]) + list(lipo_molformer_features_valid2.columns[1:])\n",
    "\n",
    "# do the same for the test set\n",
    "# standardize the labels first\n",
    "lipo_y_ensemble_test_std = (lipo_chemberta2_test['target']  - train_mean)/train_sd\n",
    "\n",
    "lipo_X_ensemble_test = pd.concat([\n",
    "    lipo_chemberta2_test['pred_z'] - lipo_y_ensemble_test_std,\n",
    "    lipo_molformer_test['pred_z'] - lipo_y_ensemble_test_std,  \n",
    "    # add features from test set\n",
    "    lipo_chemberta2_features_test.iloc[:, 2:],\n",
    "    lipo_molformer_features_test.iloc[:, 1:],\n",
    "], axis=1)\n",
    "\n",
    "# change feature names of the ensemble so that they are unique\n",
    "lipo_X_ensemble_test.columns = ['chemberta', 'molformer'] + list(lipo_chemberta2_features_test.columns[2:]) + list(lipo_molformer_features_test.columns[1:])\n",
    "\n",
    "# Use the actual labels from any of the models (assuming they are all the same across datasets)\n",
    "lipo_y_ensemble_test = lipo_chemberta2_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "lipo_X_ensemble_valid2_scaled = scaler.fit_transform(lipo_X_ensemble_valid2)\n",
    "lipo_X_ensemble_test_scaled = scaler.transform(lipo_X_ensemble_test)\n",
    "\n",
    "lipo_X_ensemble_valid2_scaled = pd.DataFrame(lipo_X_ensemble_valid2_scaled, columns=lipo_X_ensemble_valid2.columns)\n",
    "lipo_X_ensemble_test_scaled = pd.DataFrame(lipo_X_ensemble_test_scaled, columns=lipo_X_ensemble_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export lipo_X_ensemble_valid2 and lipo_y_ensemble_valid2 to csv\n",
    "lipo_X_ensemble_valid2_scaled.to_csv('./processed_data/lipo_X_ensemble_valid2_scaled_chemberta+molformer_rawres.csv', index=False)\n",
    "lipo_y_ensemble_valid2.to_csv('./processed_data/lipo_y_ensemble_valid2_chemberta+molformer.csv', index=False)\n",
    "\n",
    "# export lipo_X_ensemble_test and lipo_y_ensemble_test to csv\n",
    "lipo_X_ensemble_test_scaled.to_csv('./processed_data/lipo_X_ensemble_test_scaled_chemberta+molformer_rawres.csv', index=False)\n",
    "lipo_y_ensemble_test.to_csv('./processed_data/lipo_y_ensemble_test_chemberta+molformer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.265e-01, tolerance: 6.847e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.254e-02, tolerance: 6.802e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.023e-01, tolerance: 6.802e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.082e-01, tolerance: 6.802e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e-01, tolerance: 6.802e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.753e-02, tolerance: 6.802e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.079e-02, tolerance: 6.698e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.620e-01, tolerance: 6.698e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.397e-01, tolerance: 6.698e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.276e-01, tolerance: 6.698e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.616e-02, tolerance: 6.698e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.292e-02, tolerance: 6.698e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.746e-02, tolerance: 6.698e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.5260425946479317,\n",
       " 'RMSE': 0.6756514286213495,\n",
       " 'R2 Score': 0.683118168236633,\n",
       " 'Correlation': 0.8390353267848748}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso model\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the LassoCV model\n",
    "lasso_cv = LassoCV(cv=5, max_iter = 5000, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "lasso_cv.fit(lipo_X_ensemble_valid2_scaled, lipo_y_ensemble_valid2)\n",
    "\n",
    "# Predict the test set\n",
    "lipo_lasso_pred = lasso_cv.predict(lipo_X_ensemble_test_scaled) * train_sd + train_mean\n",
    "\n",
    "# Calculate the metrics\n",
    "lipo_lasso_metrics = {\n",
    "    \"MAE\": mean_absolute_error(lipo_y_ensemble_test, lipo_lasso_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(lipo_y_ensemble_test, lipo_lasso_pred)),\n",
    "    \"R2 Score\": r2_score(lipo_y_ensemble_test, lipo_lasso_pred),\n",
    "    \"Correlation\": pearsonr(lipo_y_ensemble_test, lipo_lasso_pred)[0]\n",
    "}\n",
    "\n",
    "lipo_lasso_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['chemberta', 'molformer', 'chemberta2_feature_5', 'chemberta2_feature_7', 'chemberta2_feature_11', 'chemberta2_feature_13', 'chemberta2_feature_18', 'chemberta2_feature_19', 'chemberta2_feature_25', 'chemberta2_feature_27', 'chemberta2_feature_28', 'chemberta2_feature_32', 'chemberta2_feature_33', 'chemberta2_feature_34', 'chemberta2_feature_37', 'chemberta2_feature_41', 'chemberta2_feature_42', 'chemberta2_feature_43', 'chemberta2_feature_46', 'chemberta2_feature_48', 'chemberta2_feature_52', 'chemberta2_feature_57', 'chemberta2_feature_61', 'chemberta2_feature_62', 'chemberta2_feature_63', 'chemberta2_feature_66', 'chemberta2_feature_67', 'chemberta2_feature_71', 'chemberta2_feature_73', 'chemberta2_feature_74', 'chemberta2_feature_78', 'chemberta2_feature_79', 'chemberta2_feature_82', 'chemberta2_feature_85', 'chemberta2_feature_86', 'chemberta2_feature_88', 'chemberta2_feature_89', 'chemberta2_feature_95', 'chemberta2_feature_96', 'chemberta2_feature_98', 'chemberta2_feature_99', 'chemberta2_feature_105', 'chemberta2_feature_106', 'chemberta2_feature_109', 'chemberta2_feature_113', 'chemberta2_feature_116', 'chemberta2_feature_119', 'chemberta2_feature_120', 'chemberta2_feature_124', 'chemberta2_feature_126', 'chemberta2_feature_127', 'chemberta2_feature_135', 'chemberta2_feature_141', 'chemberta2_feature_147', 'chemberta2_feature_157', 'chemberta2_feature_159', 'chemberta2_feature_162', 'chemberta2_feature_164', 'chemberta2_feature_168', 'chemberta2_feature_177', 'chemberta2_feature_178', 'chemberta2_feature_180', 'chemberta2_feature_184', 'chemberta2_feature_186', 'chemberta2_feature_193', 'chemberta2_feature_194', 'chemberta2_feature_195', 'chemberta2_feature_197', 'chemberta2_feature_199', 'chemberta2_feature_216', 'chemberta2_feature_225', 'chemberta2_feature_226', 'chemberta2_feature_228', 'chemberta2_feature_229', 'chemberta2_feature_230', 'chemberta2_feature_232', 'chemberta2_feature_234', 'chemberta2_feature_236', 'chemberta2_feature_237', 'chemberta2_feature_239', 'chemberta2_feature_246', 'chemberta2_feature_255', 'chemberta2_feature_256', 'chemberta2_feature_259', 'chemberta2_feature_266', 'chemberta2_feature_273', 'chemberta2_feature_275', 'chemberta2_feature_280', 'chemberta2_feature_284', 'chemberta2_feature_286', 'chemberta2_feature_287', 'chemberta2_feature_292', 'chemberta2_feature_293', 'chemberta2_feature_294', 'chemberta2_feature_296', 'chemberta2_feature_299', 'chemberta2_feature_305', 'chemberta2_feature_306', 'chemberta2_feature_307', 'chemberta2_feature_315', 'chemberta2_feature_316', 'chemberta2_feature_321', 'chemberta2_feature_323', 'chemberta2_feature_324', 'chemberta2_feature_327', 'chemberta2_feature_330', 'chemberta2_feature_335', 'chemberta2_feature_336', 'chemberta2_feature_351', 'chemberta2_feature_352', 'chemberta2_feature_353', 'chemberta2_feature_354', 'chemberta2_feature_355', 'chemberta2_feature_357', 'chemberta2_feature_370', 'chemberta2_feature_381', 'chemberta2_feature_383', 'chemberta2_feature_384', 'molformer_feature_1', 'molformer_feature_4', 'molformer_feature_5', 'molformer_feature_7', 'molformer_feature_8', 'molformer_feature_11', 'molformer_feature_12', 'molformer_feature_14', 'molformer_feature_18', 'molformer_feature_19', 'molformer_feature_23', 'molformer_feature_29', 'molformer_feature_30', 'molformer_feature_32', 'molformer_feature_33', 'molformer_feature_34', 'molformer_feature_35', 'molformer_feature_36', 'molformer_feature_37', 'molformer_feature_38', 'molformer_feature_40', 'molformer_feature_45', 'molformer_feature_47', 'molformer_feature_50', 'molformer_feature_55', 'molformer_feature_58', 'molformer_feature_62', 'molformer_feature_68', 'molformer_feature_70', 'molformer_feature_73', 'molformer_feature_76', 'molformer_feature_82', 'molformer_feature_85', 'molformer_feature_87', 'molformer_feature_88', 'molformer_feature_89', 'molformer_feature_90', 'molformer_feature_93', 'molformer_feature_99', 'molformer_feature_100', 'molformer_feature_102', 'molformer_feature_106', 'molformer_feature_107', 'molformer_feature_108', 'molformer_feature_111', 'molformer_feature_112', 'molformer_feature_115', 'molformer_feature_116', 'molformer_feature_117', 'molformer_feature_118', 'molformer_feature_119', 'molformer_feature_120', 'molformer_feature_121', 'molformer_feature_122', 'molformer_feature_125', 'molformer_feature_129', 'molformer_feature_130', 'molformer_feature_132', 'molformer_feature_133', 'molformer_feature_134', 'molformer_feature_135', 'molformer_feature_138', 'molformer_feature_141', 'molformer_feature_146', 'molformer_feature_147', 'molformer_feature_148', 'molformer_feature_151', 'molformer_feature_153', 'molformer_feature_156', 'molformer_feature_161', 'molformer_feature_162', 'molformer_feature_164', 'molformer_feature_165', 'molformer_feature_170', 'molformer_feature_171', 'molformer_feature_175', 'molformer_feature_179', 'molformer_feature_180', 'molformer_feature_181', 'molformer_feature_182', 'molformer_feature_185', 'molformer_feature_186', 'molformer_feature_190', 'molformer_feature_191', 'molformer_feature_193', 'molformer_feature_194', 'molformer_feature_201', 'molformer_feature_208', 'molformer_feature_210', 'molformer_feature_213', 'molformer_feature_216', 'molformer_feature_219', 'molformer_feature_220', 'molformer_feature_222', 'molformer_feature_223', 'molformer_feature_224', 'molformer_feature_225', 'molformer_feature_228', 'molformer_feature_229', 'molformer_feature_231', 'molformer_feature_234', 'molformer_feature_239', 'molformer_feature_248', 'molformer_feature_257', 'molformer_feature_258', 'molformer_feature_260', 'molformer_feature_262', 'molformer_feature_263', 'molformer_feature_264', 'molformer_feature_267', 'molformer_feature_269', 'molformer_feature_270', 'molformer_feature_271', 'molformer_feature_273', 'molformer_feature_274', 'molformer_feature_275', 'molformer_feature_276', 'molformer_feature_281', 'molformer_feature_282', 'molformer_feature_285', 'molformer_feature_288', 'molformer_feature_291', 'molformer_feature_292', 'molformer_feature_293', 'molformer_feature_296', 'molformer_feature_297', 'molformer_feature_298', 'molformer_feature_300', 'molformer_feature_301', 'molformer_feature_303', 'molformer_feature_305', 'molformer_feature_308', 'molformer_feature_309', 'molformer_feature_313', 'molformer_feature_315', 'molformer_feature_316', 'molformer_feature_318', 'molformer_feature_319', 'molformer_feature_320', 'molformer_feature_321', 'molformer_feature_326', 'molformer_feature_327', 'molformer_feature_335', 'molformer_feature_337', 'molformer_feature_338', 'molformer_feature_342', 'molformer_feature_343', 'molformer_feature_345', 'molformer_feature_349', 'molformer_feature_354', 'molformer_feature_360', 'molformer_feature_363', 'molformer_feature_364', 'molformer_feature_369', 'molformer_feature_370', 'molformer_feature_372', 'molformer_feature_373', 'molformer_feature_378', 'molformer_feature_385', 'molformer_feature_386', 'molformer_feature_389', 'molformer_feature_390', 'molformer_feature_392', 'molformer_feature_394', 'molformer_feature_397', 'molformer_feature_399', 'molformer_feature_402', 'molformer_feature_404', 'molformer_feature_410', 'molformer_feature_413', 'molformer_feature_415', 'molformer_feature_418', 'molformer_feature_422', 'molformer_feature_423', 'molformer_feature_424', 'molformer_feature_426', 'molformer_feature_427', 'molformer_feature_428', 'molformer_feature_429', 'molformer_feature_432', 'molformer_feature_435', 'molformer_feature_436', 'molformer_feature_438', 'molformer_feature_439', 'molformer_feature_445', 'molformer_feature_448', 'molformer_feature_452', 'molformer_feature_454', 'molformer_feature_459', 'molformer_feature_462', 'molformer_feature_464', 'molformer_feature_466', 'molformer_feature_467', 'molformer_feature_468', 'molformer_feature_469', 'molformer_feature_470', 'molformer_feature_472', 'molformer_feature_475', 'molformer_feature_476', 'molformer_feature_477', 'molformer_feature_479', 'molformer_feature_481', 'molformer_feature_482', 'molformer_feature_483', 'molformer_feature_484', 'molformer_feature_486', 'molformer_feature_487', 'molformer_feature_491', 'molformer_feature_493', 'molformer_feature_495', 'molformer_feature_497', 'molformer_feature_499', 'molformer_feature_500', 'molformer_feature_501', 'molformer_feature_504', 'molformer_feature_509', 'molformer_feature_510', 'molformer_feature_511', 'molformer_feature_512', 'molformer_feature_515', 'molformer_feature_516', 'molformer_feature_517', 'molformer_feature_524', 'molformer_feature_525', 'molformer_feature_529', 'molformer_feature_533', 'molformer_feature_535', 'molformer_feature_538', 'molformer_feature_542', 'molformer_feature_546', 'molformer_feature_548', 'molformer_feature_549', 'molformer_feature_550', 'molformer_feature_551', 'molformer_feature_552', 'molformer_feature_555', 'molformer_feature_566', 'molformer_feature_568', 'molformer_feature_570', 'molformer_feature_572', 'molformer_feature_573', 'molformer_feature_578', 'molformer_feature_582', 'molformer_feature_583', 'molformer_feature_585', 'molformer_feature_587', 'molformer_feature_590', 'molformer_feature_592', 'molformer_feature_593', 'molformer_feature_594', 'molformer_feature_599', 'molformer_feature_601', 'molformer_feature_604', 'molformer_feature_612', 'molformer_feature_614', 'molformer_feature_616', 'molformer_feature_621', 'molformer_feature_623', 'molformer_feature_624', 'molformer_feature_625', 'molformer_feature_627', 'molformer_feature_628', 'molformer_feature_631', 'molformer_feature_634', 'molformer_feature_636', 'molformer_feature_637', 'molformer_feature_639', 'molformer_feature_644', 'molformer_feature_645', 'molformer_feature_649', 'molformer_feature_650', 'molformer_feature_651', 'molformer_feature_653', 'molformer_feature_654', 'molformer_feature_655', 'molformer_feature_656', 'molformer_feature_660', 'molformer_feature_661', 'molformer_feature_662', 'molformer_feature_664', 'molformer_feature_667', 'molformer_feature_670', 'molformer_feature_671', 'molformer_feature_672', 'molformer_feature_674', 'molformer_feature_677', 'molformer_feature_678', 'molformer_feature_680', 'molformer_feature_683', 'molformer_feature_685', 'molformer_feature_686', 'molformer_feature_690', 'molformer_feature_691', 'molformer_feature_697', 'molformer_feature_698', 'molformer_feature_701', 'molformer_feature_702', 'molformer_feature_703', 'molformer_feature_705', 'molformer_feature_706', 'molformer_feature_707', 'molformer_feature_708', 'molformer_feature_709', 'molformer_feature_712', 'molformer_feature_713', 'molformer_feature_715', 'molformer_feature_722', 'molformer_feature_728', 'molformer_feature_729', 'molformer_feature_732', 'molformer_feature_734', 'molformer_feature_737', 'molformer_feature_740', 'molformer_feature_744', 'molformer_feature_746', 'molformer_feature_747', 'molformer_feature_748', 'molformer_feature_752', 'molformer_feature_754', 'molformer_feature_761', 'molformer_feature_763', 'molformer_feature_767']\n",
      "Number of Chemberta2 Features Selected: 117\n",
      "Number of Molformer Features Selected: 323\n"
     ]
    }
   ],
   "source": [
    "coefs = pd.Series(lasso_cv.coef_, index=lipo_X_ensemble_valid2.columns)\n",
    "\n",
    "selected_features = coefs[coefs != 0].index.tolist()\n",
    "\n",
    "# count the number of selected features with chemberta2, molformer, molbert, respectively\n",
    "print(\"Selected Features:\", selected_features)\n",
    "print(\"Number of Chemberta2 Features Selected:\", sum('chemberta' in feature for feature in selected_features))\n",
    "print(\"Number of Molformer Features Selected:\", sum('molformer' in feature for feature in selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.05994842503189409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.5249756281325839,\n",
       " 'RMSE': 0.661664622247795,\n",
       " 'R2 Score': 0.696102050271042,\n",
       " 'Correlation': 0.84009892018888}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skglm import GroupLasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# two groups: one for the prediction probabilities and one for the features\n",
    "\n",
    "# Define the groups for each feature\n",
    "n_features = lipo_X_ensemble_valid2_scaled.shape[1]\n",
    "\n",
    "groups = [\n",
    "    list(range(0, 2)),  # Group 0 with feature indices 0, 1, 2\n",
    "    list(range(2, n_features))  # Group 1 with all remaining features\n",
    "]\n",
    "\n",
    "# Initialize the GroupLasso model\n",
    "group_lasso_model = GroupLasso(\n",
    "    groups=groups,\n",
    "    alpha=1.0,\n",
    "    p0=10,\n",
    "    verbose=0,\n",
    "    tol=0.0001,\n",
    "    positive=False,\n",
    "    fit_intercept=True,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Setup cross-validation to find the best alpha\n",
    "param_grid = {'alpha': np.logspace(-4, 1, 10)}\n",
    "cv = GridSearchCV(\n",
    "    estimator=group_lasso_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "cv.fit(lipo_X_ensemble_valid2_scaled, lipo_y_ensemble_valid2)\n",
    "\n",
    "# Best model and parameters\n",
    "best_model = cv.best_estimator_\n",
    "print(\"Best alpha:\", cv.best_params_['alpha'])\n",
    "\n",
    "# Predict using the best model\n",
    "lipo_pred = best_model.predict(lipo_X_ensemble_test_scaled) * train_sd + train_mean\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "lipo_two_groups_lasso_best_metrics = {\n",
    "    \"MAE\": mean_absolute_error(lipo_y_ensemble_test, lipo_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(lipo_y_ensemble_test, lipo_pred)),\n",
    "    \"R2 Score\": r2_score(lipo_y_ensemble_test, lipo_pred),\n",
    "    \"Correlation\": pearsonr(lipo_y_ensemble_test, lipo_pred)[0]  # Only record the correlation coefficient\n",
    "}\n",
    "\n",
    "# Print the calculated metrics\n",
    "lipo_two_groups_lasso_best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.05994842503189409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.5217432246290242,\n",
       " 'RMSE': 0.6527466215728963,\n",
       " 'R2 Score': 0.7042387953043184,\n",
       " 'Correlation': 0.8415255212889469}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# four groups: predictions, features from chemberta, features from molformer\n",
    "\n",
    "# Define the groups for each feature\n",
    "n_features = lipo_X_ensemble_valid2_scaled.shape[1]\n",
    "groups = [\n",
    "    list(range(0, 2)),  # Group 0 with feature indices 0, 1, 2\n",
    "    list(range(2, 2 + 384)),  # Group 1 with next 384 features\n",
    "    list(range(2 + 384, n_features)),  # Group 2 with next 768 features\n",
    "]\n",
    "\n",
    "# Initialize the GroupLasso model\n",
    "group_lasso_model = GroupLasso(\n",
    "    groups=groups,\n",
    "    alpha=1.0,\n",
    "    p0=10,\n",
    "    verbose=0,\n",
    "    tol=0.0001,\n",
    "    positive=False,\n",
    "    fit_intercept=True,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Setup cross-validation to find the best alpha\n",
    "param_grid = {'alpha': np.logspace(-4, 1, 10)}\n",
    "cv = GridSearchCV(\n",
    "    estimator=group_lasso_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "cv.fit(lipo_X_ensemble_valid2_scaled, lipo_y_ensemble_valid2)\n",
    "\n",
    "# Best model and parameters\n",
    "best_model = cv.best_estimator_\n",
    "print(\"Best alpha:\", cv.best_params_['alpha'])\n",
    "\n",
    "# Predict using the best model\n",
    "lipo_pred = best_model.predict(lipo_X_ensemble_test_scaled) * train_sd + train_mean\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "lipo_three_groups_lasso_best_metrics = {\n",
    "    \"MAE\": mean_absolute_error(lipo_y_ensemble_test, lipo_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(lipo_y_ensemble_test, lipo_pred)),\n",
    "    \"R2 Score\": r2_score(lipo_y_ensemble_test, lipo_pred),\n",
    "    \"Correlation\": pearsonr(lipo_y_ensemble_test, lipo_pred)[0]  # Only record the correlation coefficient\n",
    "}\n",
    "\n",
    "# Print the calculated metrics\n",
    "lipo_three_groups_lasso_best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.087e-01, tolerance: 6.802e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.256e+00, tolerance: 6.847e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+00, tolerance: 6.800e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.490e+00, tolerance: 6.802e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/lorrainelu/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.745e+00, tolerance: 6.698e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01, 'l1_ratio': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.511505837245371,\n",
       " 'RMSE': 0.6460857536409419,\n",
       " 'R2 Score': 0.7102441113657292,\n",
       " 'Correlation': 0.8445237898913979}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elastic net\n",
    "# Define the model with elasticnet penalty for regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "elastic_net_model = ElasticNet(random_state=0, max_iter=5000)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "# Use fewer discrete values for alpha and l1_ratio\n",
    "alphas = [0.01, 0.1, 1, 3]  # Reduced number of points focusing on lower and mid-range\n",
    "l1_ratios = [0.1, 0.5, 0.9]  # Reduced to three points, emphasizing edges and midpoint\n",
    "\n",
    "params = {\n",
    "    'alpha': alphas,  # Convert alpha back to C\n",
    "    'l1_ratio': l1_ratios\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(elastic_net_model, param_grid=params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(lipo_X_ensemble_valid2_scaled, lipo_y_ensemble_valid2)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "lipo_best_elastic_params = grid_search.best_params_\n",
    "print(lipo_best_elastic_params)\n",
    "\n",
    "# Initialize and train the best ElasticNet model\n",
    "lipo_best_elastic_model = ElasticNet(alpha=lipo_best_elastic_params['alpha'], l1_ratio=lipo_best_elastic_params['l1_ratio'], random_state=0, max_iter=5000)\n",
    "lipo_best_elastic_model.fit(lipo_X_ensemble_valid2_scaled, lipo_y_ensemble_valid2)\n",
    "\n",
    "# Predict the test set\n",
    "lipo_elastic_pred = lipo_best_elastic_model.predict(lipo_X_ensemble_test_scaled) * train_sd + train_mean\n",
    "\n",
    "# Calculate the metrics\n",
    "lipo_elastic_metrics = {\n",
    "    \"MAE\": mean_absolute_error(lipo_y_ensemble_test, lipo_elastic_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(lipo_y_ensemble_test, lipo_elastic_pred)),\n",
    "    \"R2 Score\": r2_score(lipo_y_ensemble_test, lipo_elastic_pred),\n",
    "    \"Correlation\": pearsonr(lipo_y_ensemble_test, lipo_elastic_pred)[0]\n",
    "}\n",
    "\n",
    "lipo_elastic_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840, 1154)\n",
      "(420, 1154)\n"
     ]
    }
   ],
   "source": [
    "lipo_X_ensemble_valid2_selected = lipo_X_ensemble_valid2_scaled\n",
    "lipo_X_ensemble_test_selected = lipo_X_ensemble_test_scaled\n",
    "\n",
    "# check shapes\n",
    "print(lipo_X_ensemble_valid2_selected.shape)\n",
    "print(lipo_X_ensemble_test_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.6024343137759439,\n",
       " 'RMSE': 0.7615854057673571,\n",
       " 'R2 Score': 0.5973857266648716,\n",
       " 'Correlation': 0.8042425232619876}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and train the SVR model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "lipo_svr_model = SVR()\n",
    "lipo_svr_model.fit(lipo_X_ensemble_valid2_selected, lipo_y_ensemble_valid2)\n",
    "\n",
    "# Predict the test set\n",
    "lipo_svr_pred = lipo_svr_model.predict(lipo_X_ensemble_test_selected) * train_sd + train_mean\n",
    "\n",
    "# Calculate the metrics\n",
    "lipo_svr_metrics = {\n",
    "    \"MAE\": mean_absolute_error(lipo_y_ensemble_test, lipo_svr_pred ),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(lipo_y_ensemble_test, lipo_svr_pred )),\n",
    "    \"R2 Score\": r2_score(lipo_y_ensemble_test, lipo_svr_pred ),\n",
    "    \"Correlation\": pearsonr(lipo_y_ensemble_test, lipo_svr_pred )[0]  # Only record the correlation coefficient\n",
    "}\n",
    "\n",
    "lipo_svr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.766704761904762,\n",
       " 'RMSE': 0.9445742983960956,\n",
       " 'R2 Score': 0.3806670447138205,\n",
       " 'Correlation': 0.6555430594373179}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initailize and use a 5-fold cross-validation to tune the hyperparameters of a random forest model for regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "lipo_rf_model = RandomForestRegressor(random_state=0)\n",
    "\n",
    "lipo_rf_model.fit(lipo_X_ensemble_valid2_selected, lipo_y_ensemble_valid2)\n",
    "\n",
    "# Predict the test set\n",
    "lipo_rf_best_pred = lipo_rf_model.predict(lipo_X_ensemble_test_selected) * train_sd + train_mean\n",
    "\n",
    "# Calculate the metrics\n",
    "lipo_rf_best_metrics = {\n",
    "    \"MAE\": mean_absolute_error(lipo_y_ensemble_test, lipo_rf_best_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(lipo_y_ensemble_test, lipo_rf_best_pred)),\n",
    "    \"R2 Score\": r2_score(lipo_y_ensemble_test, lipo_rf_best_pred),\n",
    "    \"Correlation\": pearsonr(lipo_y_ensemble_test, lipo_rf_best_pred)[0]  # Only record the correlation coefficient\n",
    "}\n",
    "\n",
    "lipo_rf_best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [04:14<22:15, 15.90s/trial, best loss: 0.7169354876914394]\n",
      "Best hyperparameters: {'colsample_bytree': 0.7331550241973614, 'learning_rate': 0.05108990813691023, 'max_depth': 6.0, 'n_estimators': 150.0, 'subsample': 0.7169658536717507}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "# Define the hyperparameter space using continuous distributions\n",
    "lipo_xgb_hyperopt_space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 50),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 7, 2),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "# Correctly define the RMSE scorer function\n",
    "def rmse_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Convert float outputs of hp.quniform to int for certain parameters\n",
    "def objective(params):\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    model = xgb.XGBRegressor(**params, random_state=0)\n",
    "    \n",
    "    # Cross-validated RMSE as the objective\n",
    "    score = cross_val_score(model, lipo_X_ensemble_valid2_selected, lipo_y_ensemble_valid2, \n",
    "                            scoring=make_scorer(rmse_scorer, greater_is_better=False), cv=5)\n",
    "    \n",
    "    # Minimize the positive RMSE (already negative from scoring)\n",
    "    return {'loss': -score.mean(), 'status': STATUS_OK}\n",
    "\n",
    "# Run the Bayesian optimization\n",
    "trials = Trials()\n",
    "lipo_xgb_best_params = fmin(fn=objective, \n",
    "                            space=lipo_xgb_hyperopt_space, \n",
    "                            algo=tpe.suggest, \n",
    "                            max_evals=100, \n",
    "                            trials=trials,\n",
    "                            early_stop_fn=no_progress_loss(10))\n",
    "\n",
    "print(\"Best hyperparameters:\", lipo_xgb_best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.6760608920483362,\n",
       " 'RMSE': 0.8440640100681254,\n",
       " 'R2 Score': 0.5054585398377907,\n",
       " 'Correlation': 0.7384327498523273}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model with the best hyperparameters\n",
    "# Convert parameters obtained from Hyperopt to the correct data type\n",
    "lipo_xgb_best_params['n_estimators'] = int(lipo_xgb_best_params['n_estimators'])\n",
    "lipo_xgb_best_params['max_depth'] = int(lipo_xgb_best_params['max_depth'])\n",
    "\n",
    "# Initialize and train the XGBoost model with the best parameters\n",
    "lipo_xgb_model = xgb.XGBRegressor(**lipo_xgb_best_params, random_state=0)\n",
    "lipo_xgb_model.fit(lipo_X_ensemble_valid2_selected, lipo_y_ensemble_valid2)\n",
    "\n",
    "# Predict the test set\n",
    "lipo_xgb_best_pred = lipo_xgb_model.predict(lipo_X_ensemble_test_selected) * train_sd + train_mean\n",
    "\n",
    "# Calculate the metrics\n",
    "lipo_xgb_best_metrics = {\n",
    "    \"MAE\": mean_absolute_error(lipo_y_ensemble_test, lipo_xgb_best_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(lipo_y_ensemble_test, lipo_xgb_best_pred)),\n",
    "    \"R2 Score\": r2_score(lipo_y_ensemble_test, lipo_xgb_best_pred),\n",
    "    \"Correlation\": pearsonr(lipo_y_ensemble_test, lipo_xgb_best_pred)[0]  # Only record the correlation coefficient\n",
    "}\n",
    "\n",
    "lipo_xgb_best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [03:10<05:10, 10.03s/trial, best loss: 0.5671142339706421]\n",
      "Best hyperparameters: {'dropout_rate': 0.09123887228621164, 'learning_rate': 0.0007853651724204861, 'num_layers': 2.0, 'num_neurons': 223.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define RMSE loss\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))\n",
    "\n",
    "# Define the neural network model for regression\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, num_neurons, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        for _ in range(1, num_layers):\n",
    "            layers += [nn.Linear(num_neurons, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        layers += [nn.Linear(num_neurons, 1)]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Hyperparameter space with hp.quniform for integer distribution\n",
    "space = {\n",
    "    'num_layers': hp.quniform('num_layers', 1, 5, 1),\n",
    "    'num_neurons': hp.quniform('num_neurons', 16, 256, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5)\n",
    "}\n",
    "\n",
    "# Global dataset variables assumed to be defined externally\n",
    "X = lipo_X_ensemble_valid2_selected\n",
    "y = lipo_y_ensemble_valid2\n",
    "\n",
    "# Objective function for Bayesian optimization\n",
    "def objective(params):\n",
    "    params['num_layers'] = int(params['num_layers'])  # Ensure num_layers is an integer\n",
    "    params['num_neurons'] = int(params['num_neurons'])  # Ensure num_neurons is an integer\n",
    "    kf = KFold(n_splits=5)\n",
    "    rmse_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Convert DataFrame to numpy arrays before making them PyTorch tensors\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train.values.astype(np.float32)), \n",
    "                                      torch.tensor(y_train.values.astype(np.float32)).unsqueeze(1))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        model = SimpleNN(input_size=X_train.shape[1], num_layers=params['num_layers'],\n",
    "                         num_neurons=params['num_neurons'], dropout_rate=params['dropout_rate'])\n",
    "        criterion = RMSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(100):\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(torch.tensor(X_val.values.astype(np.float32))).squeeze(1)\n",
    "            val_targets = torch.tensor(y_val.values.astype(np.float32))\n",
    "            rmse = np.sqrt(mean_squared_error(val_targets.numpy(), val_preds.numpy()))\n",
    "            rmse_scores.append(rmse)\n",
    "\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    return {'loss': avg_rmse, 'status': STATUS_OK} # Minimize RMSE\n",
    "\n",
    "# Run Bayesian optimization\n",
    "trials = Trials()\n",
    "lipo_nn_best_params = fmin(fn=objective,\n",
    "                           space=space,\n",
    "                           algo=tpe.suggest,\n",
    "                           max_evals=50,\n",
    "                           trials=trials,\n",
    "                           early_stop_fn=no_progress_loss(10))\n",
    "\n",
    "print(\"Best hyperparameters:\", lipo_nn_best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.44364768,\n",
       " 'RMSE': 0.59092075,\n",
       " 'R2 Score': 0.7576124019596023,\n",
       " 'Correlation': 0.8710138434103935}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the neural network model again\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, num_neurons, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        for _ in range(1, num_layers):\n",
    "            layers += [nn.Linear(num_neurons, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        layers += [nn.Linear(num_neurons, 1)]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define a function to compute RMSE\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Convert parameters to the correct format if necessary\n",
    "lipo_nn_best_params = {\n",
    "    'num_layers':  int(lipo_nn_best_params['num_layers']),  # Extracted from Bayesian optimization results\n",
    "    'num_neurons':  int(lipo_nn_best_params['num_neurons']),  # Extracted from Bayesian optimization results\n",
    "    'dropout_rate': lipo_nn_best_params['dropout_rate'],  # Extracted from Bayesian optimization results\n",
    "    'learning_rate': lipo_nn_best_params['learning_rate']  # Extracted from Bayesian optimization results\n",
    "}\n",
    "\n",
    "# Prepare datasets\n",
    "X_train_tensor = torch.tensor(lipo_X_ensemble_valid2_selected.values.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(lipo_y_ensemble_valid2.values.astype(np.float32)).unsqueeze(1)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "X_test_tensor = torch.tensor(lipo_X_ensemble_test_selected.values.astype(np.float32))\n",
    "y_test_tensor = torch.tensor(lipo_y_ensemble_test.values.astype(np.float32)).unsqueeze(1)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN(input_size=lipo_X_ensemble_valid2_selected.shape[1], num_layers=lipo_nn_best_params['num_layers'],\n",
    "                         num_neurons=lipo_nn_best_params['num_neurons'], dropout_rate=lipo_nn_best_params['dropout_rate'])\n",
    "criterion = RMSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lipo_nn_best_params['learning_rate'])\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(100):  # Number of epochs can be adjusted\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predictions = outputs.squeeze(1).numpy() * train_sd + train_mean\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test_tensor.numpy(), predictions)\n",
    "    rmse = compute_rmse(y_test_tensor.numpy(), predictions)\n",
    "    r2 = r2_score(y_test_tensor.numpy(), predictions)\n",
    "    correlation, _ = pearsonr(y_test_tensor.numpy().squeeze(1), predictions)\n",
    "\n",
    "    lipo_nn_metrics = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2 Score': r2,\n",
    "        'Correlation': correlation\n",
    "    }\n",
    "\n",
    "lipo_nn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table to record all metrics for lipo\n",
    "lipo_metrics_results[\"LASSO\"] = lipo_lasso_metrics\n",
    "lipo_metrics_results[\"Group Lasso (2 groups)\"] = lipo_two_groups_lasso_best_metrics\n",
    "lipo_metrics_results[\"Group Lasso (3 groups)\"] = lipo_three_groups_lasso_best_metrics\n",
    "lipo_metrics_results[\"Elastic Net\"] = lipo_elastic_metrics\n",
    "lipo_metrics_results[\"SVR\"] = lipo_svr_metrics\n",
    "lipo_metrics_results[\"Random Forest\"] = lipo_rf_best_metrics\n",
    "lipo_metrics_results[\"XGBoost\"] = lipo_xgb_best_metrics\n",
    "lipo_metrics_results[\"Neural Network\"] = lipo_nn_metrics\n",
    "\n",
    "lipo_metrics_df = pd.DataFrame(lipo_metrics_results).T\n",
    "# keep 3 digits after the decimal point\n",
    "lipo_metrics_df = lipo_metrics_df.round(3)\n",
    "\n",
    "# export table to csv\n",
    "lipo_metrics_df.to_csv('./split3_lipo_metrics_chemberta+molformer_rawres.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion-env",
   "language": "python",
   "name": "fusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
