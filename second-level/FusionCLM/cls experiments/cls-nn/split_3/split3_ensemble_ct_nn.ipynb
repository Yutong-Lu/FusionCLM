{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, average_precision_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# preds\n",
    "\n",
    "# Load the training set of meta-model\n",
    "ct_chemberta2_valid2 = pd.read_csv('./chemberta2/results/clintox/chemberta2_valid2_clintox_3_predictions.csv')\n",
    "ct_molformer_valid2 = pd.read_csv('./molformer/results/clintox/molformer_valid2_clintox_3_epoch26.csv')\n",
    "ct_molbert_valid2 = pd.read_csv('./molbert/results/cilntox/molbert_valid2_clintox_3.csv')\n",
    "\n",
    "# Load the test data for each model\n",
    "ct_chemberta2_test = pd.read_csv('./chemberta2/results/clintox/chemberta2_test_clintox_3_predictions.csv')\n",
    "ct_molformer_test = pd.read_csv('./molformer/results/clintox/molformer_test_clintox_3_epoch26.csv')\n",
    "ct_molbert_test = pd.read_csv('./molbert/results/cilntox/molbert_test_clintox_3.csv')\n",
    "\n",
    "# features\n",
    "\n",
    "# Load the features from chemberta\n",
    "ct_chemberta2_features_valid2 = pd.read_csv('./chemberta2/features/clintox/chemberta2_valid2_clintox_3_features.csv')\n",
    "ct_chemberta2_features_test = pd.read_csv('./chemberta2/features/clintox/chemberta2_test_clintox_3_features.csv')\n",
    "\n",
    "# Load the features from molformer\n",
    "ct_molformer_features_valid2 = pd.read_csv('./molformer/features/clintox/molformer_valid2_clintox_3_features.csv')\n",
    "ct_molformer_features_test = pd.read_csv('./molformer/features/clintox/molformer_test_clintox_3_features.csv')\n",
    "\n",
    "# Load the features from molbert\n",
    "ct_molbert_features_valid2 = pd.read_csv('./molbert/features/clintox/molbert_valid2_clintox_3_features.csv')\n",
    "ct_molbert_features_test = pd.read_csv('./molbert/features/clintox/molbert_test_clintox_3_features.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Clintox (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chemberta2': {'Accuracy': 0.9731543624161074,\n",
       "  'F1 Score': 0.8,\n",
       "  'ROC-AUC': 0.9964028776978417,\n",
       "  'PR-AUC': 0.9540404040404039},\n",
       " 'Molformer': {'Accuracy': 0.9328859060402684,\n",
       "  'F1 Score': 0.375,\n",
       "  'ROC-AUC': 0.9266187050359712,\n",
       "  'PR-AUC': 0.5340980864486556},\n",
       " 'Molbert': {'Accuracy': 0.9172413793103448,\n",
       "  'F1 Score': 0.14285714285714285,\n",
       "  'ROC-AUC': 0.9133333333333333,\n",
       "  'PR-AUC': 0.46665875863902184}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Preparing the actual and predicted values\n",
    "# Chemberta2\n",
    "ct_chemberta_actual = ct_chemberta2_test['CT_TOX']\n",
    "ct_chemberta_pred = ct_chemberta2_test['y_pred']\n",
    "ct_chemberta_probs = ct_chemberta2_test[['softmax_class_0_prob', 'softmax_class_1_prob']]\n",
    "\n",
    "# Molformer\n",
    "ct_molformer_actual = ct_molformer_test['Actual']\n",
    "ct_molformer_pred = (ct_molformer_test['Prob_Class_1'] > 0.5).astype(int)\n",
    "ct_molformer_probs = ct_molformer_test[['Prob_Class_0', 'Prob_Class_1']]\n",
    "\n",
    "# Molbert\n",
    "ct_molbert_actual = ct_molbert_test['target']\n",
    "ct_molbert_pred = ct_molbert_test['pred']\n",
    "ct_molbert_probs = ct_molbert_test['prob']\n",
    "\n",
    "# Calculating metrics\n",
    "ct_metrics_results = {}\n",
    "\n",
    "for model_name, actual, pred, probs in [(\"Chemberta2\", ct_chemberta_actual, ct_chemberta_pred, ct_chemberta_probs['softmax_class_1_prob']),\n",
    "                                         (\"Molformer\", ct_molformer_actual, ct_molformer_pred, ct_molformer_probs['Prob_Class_1']),\n",
    "                                         (\"Molbert\", ct_molbert_actual, ct_molbert_pred, ct_molbert_probs)]:\n",
    "    ct_metrics_results[model_name] = {\n",
    "        \"Accuracy\": accuracy_score(actual, pred),\n",
    "        \"F1 Score\": f1_score(actual, pred),\n",
    "        \"ROC-AUC\": roc_auc_score(actual, probs),\n",
    "        \"PR-AUC\": average_precision_score(actual, probs)\n",
    "    }\n",
    "\n",
    "ct_metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the 'smiles' values in chemberta that are not in molbert\n",
    "missing_smiles = set(ct_chemberta2_valid2['smiles']) - set(ct_molbert_valid2['smiles'])\n",
    "\n",
    "# Find the indices of these missing 'smiles' in the chemberta DataFrame\n",
    "missing_indices = ct_chemberta2_valid2.index[ct_chemberta2_valid2['smiles'].isin(missing_smiles)].tolist()\n",
    "\n",
    "# Drop the rows with these missing indices for chemberta and molformer\n",
    "ct_chemberta2_valid2 = ct_chemberta2_valid2.drop(missing_indices)\n",
    "ct_molformer_valid2 = ct_molformer_valid2.drop(missing_indices)\n",
    "ct_chemberta2_features_valid2 = ct_chemberta2_features_valid2.drop(missing_indices)\n",
    "ct_molformer_features_valid2 = ct_molformer_features_valid2.drop(missing_indices)\n",
    "ct_molbert_features_valid2 = ct_molbert_features_valid2.drop(missing_indices)\n",
    "\n",
    "# do the same for test sets\n",
    "missing_smiles = set(ct_chemberta2_test['smiles']) - set(ct_molbert_test['smiles'])\n",
    "missing_indices = ct_chemberta2_test.index[ct_chemberta2_test['smiles'].isin(missing_smiles)].tolist()\n",
    "\n",
    "ct_chemberta2_test = ct_chemberta2_test.drop(missing_indices)\n",
    "ct_molformer_test = ct_molformer_test.drop(missing_indices)\n",
    "ct_chemberta2_features_test = ct_chemberta2_features_test.drop(missing_indices)\n",
    "ct_molformer_features_test = ct_molformer_features_test.drop(missing_indices)\n",
    "ct_molbert_features_test = ct_molbert_features_test.drop(missing_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295, 8)\n",
      "(295, 5)\n",
      "(295, 4)\n",
      "(295, 386)\n",
      "(295, 769)\n",
      "(295, 769)\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "print(ct_chemberta2_valid2.shape)\n",
    "print(ct_molformer_valid2.shape)\n",
    "print(ct_molbert_valid2.shape)\n",
    "print(ct_chemberta2_features_valid2.shape)\n",
    "print(ct_molformer_features_valid2.shape)\n",
    "print(ct_molbert_features_valid2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ct_y_ensemble_valid2 = ct_chemberta2_valid2['CT_TOX']\n",
    "\n",
    "# Convert the ensemble target to a Series if not already done\n",
    "ct_y_ensemble_valid2_s = pd.Series(ct_y_ensemble_valid2).reset_index(drop=True)\n",
    "\n",
    "# Create dataframes for each model's class 1 probability\n",
    "ct_chemberta2_prob = pd.DataFrame({'chemberta2': ct_chemberta2_valid2['softmax_class_1_prob']})\n",
    "ct_chemberta2_prob.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_molformer_prob = pd.DataFrame({'molformer': ct_molformer_valid2['Prob_Class_1']})\n",
    "ct_molformer_prob.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_molbert_prob = pd.DataFrame({'molbert': ct_molbert_valid2['prob']})\n",
    "ct_molbert_prob.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# do the same for features ct_chemberta2_features_valid2.iloc[:, 2:]\n",
    "ct_chemberta2_features = pd.DataFrame(ct_chemberta2_features_valid2.iloc[:, 2:])\n",
    "ct_chemberta2_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_molformer_features = pd.DataFrame(ct_molformer_features_valid2.iloc[:, 1:])\n",
    "ct_molformer_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_molbert_features = pd.DataFrame(ct_molbert_features_valid2.iloc[:, 1:])\n",
    "ct_molbert_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ct_features = pd.concat([ct_chemberta2_features, ct_molformer_features, ct_molbert_features], axis=1)\n",
    "\n",
    "# Combine probabilities into one dataframe\n",
    "train_ct_prob = pd.concat([ct_chemberta2_prob, ct_molformer_prob, ct_molbert_prob], axis=1)\n",
    "\n",
    "# Function to calculate BCE for each row\n",
    "def calculate_bce_rowwise(y_true, y_pred):\n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Calculate row-wise BCE for each model\n",
    "bce_chemberta = calculate_bce_rowwise(ct_y_ensemble_valid2_s, ct_chemberta2_prob['chemberta2'])\n",
    "bce_molformer = calculate_bce_rowwise(ct_y_ensemble_valid2_s, ct_molformer_prob['molformer'])\n",
    "bce_molbert = calculate_bce_rowwise(ct_y_ensemble_valid2_s, ct_molbert_prob['molbert'])\n",
    "\n",
    "# Create a dataframe for row-wise BCE losses\n",
    "bce_loss_df = pd.DataFrame({\n",
    "    'bce_chemberta': bce_chemberta,\n",
    "    'bce_molformer': bce_molformer,\n",
    "    'bce_molbert': bce_molbert\n",
    "})\n",
    "\n",
    "# Final ensemble X matrix: Combine row-wise BCE losses, predictions, and features\n",
    "ct_X_ensemble_valid2 = pd.concat([bce_loss_df, train_ct_prob], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for chemberta...\n",
      " 32%|███▏      | 16/50 [01:05<02:18,  4.08s/trial, best loss: 0.3201264441013336]\n",
      "Best hyperparameters for chemberta: {'dropout_rate': 0.3476486742192898, 'learning_rate': 0.004207657605324512, 'num_layers': 3.0, 'num_neurons': 256.0}\n",
      "Optimizing for molformer...\n",
      " 40%|████      | 20/50 [01:27<02:11,  4.38s/trial, best loss: 1.6076781749725342]\n",
      "Best hyperparameters for molformer: {'dropout_rate': 0.12139934126762336, 'learning_rate': 0.006175152235539875, 'num_layers': 3.0, 'num_neurons': 96.0}\n",
      "Optimizing for molbert...\n",
      " 32%|███▏      | 16/50 [01:09<02:27,  4.35s/trial, best loss: 0.8250967264175415]\n",
      "Best hyperparameters for molbert: {'dropout_rate': 0.3476486742192898, 'learning_rate': 0.004207657605324512, 'num_layers': 3.0, 'num_neurons': 256.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "# Combine probabilities with their respective feature sets\n",
    "chemberta_X = pd.concat([ct_chemberta2_prob, ct_chemberta2_features], axis=1)\n",
    "molformer_X = pd.concat([ct_molformer_prob, ct_molformer_features], axis=1)\n",
    "molbert_X = pd.concat([ct_molbert_prob, ct_molbert_features], axis=1)\n",
    "\n",
    "# Standardize each dataset\n",
    "scaler_chemberta = StandardScaler().fit(chemberta_X)\n",
    "scaler_molformer = StandardScaler().fit(molformer_X)\n",
    "scaler_molbert = StandardScaler().fit(molbert_X)\n",
    "\n",
    "chemberta_X_scaled = scaler_chemberta.transform(chemberta_X)\n",
    "molformer_X_scaled = scaler_molformer.transform(molformer_X)\n",
    "molbert_X_scaled = scaler_molbert.transform(molbert_X)\n",
    "\n",
    "# Define the binary cross-entropy loss values as target variables (y)\n",
    "chemberta_y_bce = bce_chemberta  # Row-wise BCE loss calculated earlier\n",
    "molformer_y_bce = bce_molformer  # Row-wise BCE loss calculated earlier\n",
    "molbert_y_bce = bce_molbert      # Row-wise BCE loss calculated earlier\n",
    "\n",
    "# Ensure that X_scaled and y_bce are numpy arrays\n",
    "chemberta_X_scaled = np.array(chemberta_X_scaled)\n",
    "molformer_X_scaled = np.array(molformer_X_scaled)\n",
    "molbert_X_scaled = np.array(molbert_X_scaled)\n",
    "\n",
    "chemberta_y_bce = np.array(chemberta_y_bce)\n",
    "molformer_y_bce = np.array(molformer_y_bce)\n",
    "molbert_y_bce = np.array(molbert_y_bce)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define custom RMSE loss function\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, num_neurons, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            layers += [nn.Linear(num_neurons, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        layers += [nn.Linear(num_neurons, 1)]  # Output is continuous, no activation function\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Objective function for Bayesian optimization\n",
    "def objective(params, X, y):\n",
    "    kf = KFold(n_splits=5)\n",
    "    rmses = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train.astype(np.float32)), \n",
    "                                      torch.tensor(y_train.astype(np.float32)).unsqueeze(1))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        model = SimpleNN(input_size=X_train.shape[1], num_layers=int(params['num_layers']), \n",
    "                         num_neurons=int(params['num_neurons']), dropout_rate=params['dropout_rate'])\n",
    "        criterion = RMSELoss()  # Use RMSELoss for regression\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(100):  # Set number of epochs\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val.astype(np.float32))\n",
    "            y_val_tensor = torch.tensor(y_val.astype(np.float32)).unsqueeze(-1)\n",
    "            outputs = model(X_val_tensor)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_tensor.numpy(), outputs.numpy()))\n",
    "            rmses.append(rmse)\n",
    "\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    return {'loss': avg_rmse, 'status': STATUS_OK}  # Minimize RMSE\n",
    "\n",
    "# Hyperparameter space for optimization\n",
    "space = {\n",
    "    'num_layers': hp.quniform('num_layers', 1, 5, 1),\n",
    "    'num_neurons': hp.quniform('num_neurons', 16, 256, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5)\n",
    "}\n",
    "\n",
    "# Scaled input data (after standardization) and corresponding target (binary cross-entropy loss)\n",
    "datasets = {\n",
    "    'chemberta': (chemberta_X_scaled, chemberta_y_bce),\n",
    "    'molformer': (molformer_X_scaled, molformer_y_bce),\n",
    "    'molbert': (molbert_X_scaled, molbert_y_bce)\n",
    "}\n",
    "\n",
    "# Run Bayesian optimization for each model\n",
    "best_params = {}\n",
    "trials_dict = {}\n",
    "\n",
    "for dataset_name, (X_scaled, y_bce) in datasets.items():\n",
    "    print(f\"Optimizing for {dataset_name}...\")\n",
    "    trials = Trials()\n",
    "    \n",
    "    best_params[dataset_name] = fmin(fn=lambda params: objective(params, X_scaled, y_bce),\n",
    "                                     space=space,\n",
    "                                     algo=tpe.suggest,\n",
    "                                     max_evals=50,\n",
    "                                     trials=trials,\n",
    "                                     rstate=np.random.default_rng(0),  # Seed for reproducibility in hyperopt\n",
    "                                     early_stop_fn=no_progress_loss(10))\n",
    "    \n",
    "    trials_dict[dataset_name] = trials\n",
    "    print(f\"Best hyperparameters for {dataset_name}: {best_params[dataset_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Custom RMSE loss function\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(self.mse(y_pred, y_true))\n",
    "\n",
    "# Neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, num_neurons, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            layers += [nn.Linear(num_neurons, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        layers += [nn.Linear(num_neurons, 1)]  # Output layer\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Function to train the model using the best hyperparameters\n",
    "def train_model(X_train, y_train, best_params):\n",
    "    # Create DataLoader for training data\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train.astype(np.float32)), \n",
    "                                  torch.tensor(y_train.astype(np.float32)).unsqueeze(1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Initialize the model with the best hyperparameters\n",
    "    model = SimpleNN(input_size=X_train.shape[1],\n",
    "                     num_layers=int(best_params['num_layers']),\n",
    "                     num_neurons=int(best_params['num_neurons']),\n",
    "                     dropout_rate=best_params['dropout_rate'])\n",
    "    \n",
    "    # Define the loss and optimizer\n",
    "    criterion = RMSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(100):  # Training for 100 epochs\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load your best hyperparameters from the optimization step\n",
    "# Replace these with the actual best hyperparameters you found\n",
    "best_params_chemberta = best_params['chemberta']\n",
    "best_params_molformer = best_params['molformer']\n",
    "best_params_molbert = best_params['molbert']\n",
    "\n",
    "# Train the models for each dataset\n",
    "best_model_chemberta = train_model(chemberta_X_scaled, chemberta_y_bce, best_params_chemberta)\n",
    "best_model_molformer = train_model(molformer_X_scaled, molformer_y_bce, best_params_molformer)\n",
    "best_model_molbert = train_model(molbert_X_scaled, molbert_y_bce, best_params_molbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Test data for each model\n",
    "ct_chemberta2_prob_test = pd.DataFrame({'chemberta2': ct_chemberta2_test['softmax_class_1_prob']})\n",
    "ct_chemberta2_prob_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_molformer_prob_test = pd.DataFrame({'molformer': ct_molformer_test['Prob_Class_1']})\n",
    "ct_molformer_prob_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_molbert_prob_test = pd.DataFrame({'molbert': ct_molbert_test['prob']})\n",
    "ct_molbert_prob_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_chemberta2_features_t = pd.DataFrame(ct_chemberta2_features_test.iloc[:, 2:])\n",
    "ct_chemberta2_features_t.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_molformer_features_t  = pd.DataFrame(ct_molformer_features_test.iloc[:, 1:])\n",
    "ct_molformer_features_t.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ct_molbert_features_t = pd.DataFrame(ct_molbert_features_test.iloc[:, 1:])\n",
    "ct_molbert_features_t.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine probabilities with the respective feature sets for the test set\n",
    "chemberta_X_test = pd.concat([ct_chemberta2_prob_test, ct_chemberta2_features_t], axis=1)\n",
    "molformer_X_test = pd.concat([ct_molformer_prob_test, ct_molformer_features_t], axis=1)\n",
    "molbert_X_test = pd.concat([ct_molbert_prob_test, ct_molbert_features_t], axis=1)\n",
    "\n",
    "# Standardize the test set based on the previously fitted scalers\n",
    "chemberta_X_test_scaled = scaler_chemberta.transform(chemberta_X_test)\n",
    "molformer_X_test_scaled = scaler_molformer.transform(molformer_X_test)\n",
    "molbert_X_test_scaled = scaler_molbert.transform(molbert_X_test)\n",
    "\n",
    "# Ensure that X_scaled and y_bce are numpy arrays\n",
    "chemberta_X_test_scaled= np.array(chemberta_X_test_scaled)\n",
    "molformer_X_test_scaled = np.array(molformer_X_test_scaled)\n",
    "molbert_X_test_scaled = np.array(molbert_X_test_scaled)\n",
    "\n",
    "# Predict on the test sets (assuming your test sets are already preprocessed and scaled)\n",
    "with torch.no_grad():\n",
    "    chemberta_X_test_tensor = torch.tensor(chemberta_X_test_scaled.astype(np.float32))\n",
    "    molformer_X_test_tensor = torch.tensor(molformer_X_test_scaled.astype(np.float32))\n",
    "    molbert_X_test_tensor = torch.tensor(molbert_X_test_scaled.astype(np.float32))\n",
    "    \n",
    "    chemberta_pred_test = best_model_chemberta(chemberta_X_test_tensor).numpy()\n",
    "    molformer_pred_test = best_model_molformer(molformer_X_test_tensor).numpy()\n",
    "    molbert_pred_test = best_model_molbert(molbert_X_test_tensor).numpy()\n",
    "        \n",
    "    # Ensure the predictions are 1-dimensional by flattening if necessary\n",
    "    chemberta_pred_test = chemberta_pred_test.flatten()\n",
    "    molformer_pred_test = molformer_pred_test.flatten()\n",
    "    molbert_pred_test = molbert_pred_test.flatten()\n",
    "    \n",
    "# The predictions are now stored in chemberta_pred_test, molformer_pred_test, molbert_pred_test\n",
    "\n",
    "# Convert the predictions (numpy arrays) to pandas Series\n",
    "chemberta_pred_test_series = pd.Series(chemberta_pred_test, name='bce_chemberta')\n",
    "molformer_pred_test_series = pd.Series(molformer_pred_test, name='bce_molformer')\n",
    "molbert_pred_test_series = pd.Series(molbert_pred_test, name='bce_molbert')\n",
    "\n",
    "# Now concatenate the series with the test set probabilities\n",
    "ct_X_ensemble_test = pd.concat([\n",
    "    chemberta_pred_test_series,                     # BCE for Chemberta\n",
    "    molformer_pred_test_series,                     # BCE for Molformer\n",
    "    molbert_pred_test_series,                       # BCE for Molbert\n",
    "    ct_chemberta2_prob_test['chemberta2'],        # Chemberta test probabilities\n",
    "    ct_molformer_prob_test['molformer'],          # Molformer test probabilities\n",
    "    ct_molbert_prob_test['molbert']               # Molbert test probabilities\n",
    "], axis=1)\n",
    "\n",
    "ct_X_ensemble_test.columns = ['bce_chemberta', 'bce_molformer', 'bce_molbert', 'chemberta2', 'molformer', 'molbert']\n",
    "\n",
    "# optional for evaluation\n",
    "ct_y_ensemble_test = ct_chemberta2_test['CT_TOX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ct_X_ensemble_valid2_scaled = scaler.fit_transform(ct_X_ensemble_valid2)\n",
    "ct_X_ensemble_test_scaled = scaler.transform(ct_X_ensemble_test)\n",
    "\n",
    "# transform back to dataframe\n",
    "ct_X_ensemble_valid2_scaled = pd.DataFrame(ct_X_ensemble_valid2_scaled, columns=ct_X_ensemble_valid2.columns)\n",
    "ct_X_ensemble_test_scaled = pd.DataFrame(ct_X_ensemble_test_scaled, columns=ct_X_ensemble_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295, 6)\n",
      "(145, 6)\n"
     ]
    }
   ],
   "source": [
    "ct_X_ensemble_valid2_selected = ct_X_ensemble_valid2_scaled\n",
    "ct_X_ensemble_test_selected = ct_X_ensemble_test_scaled\n",
    "# check shapes\n",
    "print(ct_X_ensemble_valid2_selected.shape)\n",
    "print(ct_X_ensemble_test_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:27<01:51,  2.78s/trial, best loss: -1.0]\n",
      "Best hyperparameters: {'dropout_rate': 0.4175299871186762, 'learning_rate': 0.00024043206714908388, 'num_layers': 2.0, 'num_neurons': 180.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, num_neurons, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            layers += [nn.Linear(num_neurons, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        layers += [nn.Linear(num_neurons, 1), nn.Sigmoid()]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Objective function for Bayesian optimization\n",
    "def objective(params):\n",
    "    kf = KFold(n_splits=5)\n",
    "    roc_aucs = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train.values.astype(np.float32)), \n",
    "                                      torch.tensor(y_train.values.astype(np.float32)).unsqueeze(1))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        model = SimpleNN(input_size=X_train.shape[1], num_layers=int(params['num_layers']), \n",
    "                         num_neurons=int(params['num_neurons']), dropout_rate=params['dropout_rate'])\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(100):\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val.values.astype(np.float32))\n",
    "            y_val_tensor = torch.tensor(y_val.values.astype(np.float32)).unsqueeze(-1)\n",
    "            outputs = model(X_val_tensor)\n",
    "            roc_auc = roc_auc_score(y_val_tensor.numpy(), outputs.numpy())\n",
    "            roc_aucs.append(roc_auc)\n",
    "\n",
    "    avg_roc_auc = np.mean(roc_aucs)\n",
    "    return {'loss': -avg_roc_auc, 'status': STATUS_OK}  # Maximize ROC AUC by minimizing the negative ROC AUC\n",
    "\n",
    "# Hyperparameter space\n",
    "space = {\n",
    "    'num_layers': hp.quniform('num_layers', 1, 5, 1),\n",
    "    'num_neurons': hp.quniform('num_neurons', 16, 256, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5)\n",
    "}\n",
    "\n",
    "X = ct_X_ensemble_valid2_selected\n",
    "y = ct_y_ensemble_valid2\n",
    "\n",
    "# Run Bayesian optimization\n",
    "trials = Trials()\n",
    "ct_nn_best_params = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=trials,\n",
    "            rstate=np.random.default_rng(0),  # Seed for reproducibility in hyperopt\n",
    "            early_stop_fn=no_progress_loss(10))\n",
    "\n",
    "print(\"Best hyperparameters:\", ct_nn_best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9586206896551724,\n",
       " 'F1 Score': 0.5714285714285714,\n",
       " 'ROC-AUC': 0.9992592592592593,\n",
       " 'PR-AUC': 0.9909090909090909}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the neural network model again\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, num_neurons, dropout_rate):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            layers += [nn.Linear(num_neurons, num_neurons), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        \n",
    "        layers += [nn.Linear(num_neurons, 1), nn.Sigmoid()]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Convert parameters to the correct format if necessary\n",
    "ct_nn_best_params = {\n",
    "    'num_layers': int(ct_nn_best_params['num_layers']),  # Extracted from Bayesian optimization results\n",
    "    'num_neurons': int(ct_nn_best_params['num_neurons']),  # Extracted from Bayesian optimization results\n",
    "    'dropout_rate': ct_nn_best_params['dropout_rate'],  # Extracted from Bayesian optimization results\n",
    "    'learning_rate': ct_nn_best_params['learning_rate']  # Extracted from Bayesian optimization results\n",
    "}\n",
    "\n",
    "# Prepare datasets\n",
    "X_train_tensor = torch.tensor(ct_X_ensemble_valid2_selected.values.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(ct_y_ensemble_valid2.values.astype(np.float32)).unsqueeze(1)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "X_test_tensor = torch.tensor(ct_X_ensemble_test_selected.values.astype(np.float32))\n",
    "y_test_tensor = torch.tensor(ct_y_ensemble_test.values.astype(np.float32)).unsqueeze(1)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN(input_size=ct_X_ensemble_valid2_selected.shape[1], num_layers=ct_nn_best_params['num_layers'], \n",
    "                 num_neurons=ct_nn_best_params['num_neurons'], dropout_rate=ct_nn_best_params['dropout_rate'])\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=ct_nn_best_params['learning_rate'])\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(100):  # Number of epochs can be adjusted\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predictions = (outputs > 0.5).float()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), predictions.numpy())\n",
    "    f1 = f1_score(y_test_tensor.numpy(), predictions.numpy())\n",
    "    roc_auc = roc_auc_score(y_test_tensor.numpy(), outputs.numpy())\n",
    "    pr_auc = average_precision_score(y_test_tensor.numpy(), outputs.numpy())\n",
    "\n",
    "    ct_nn_metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc\n",
    "    }\n",
    "\n",
    "ct_nn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report all the metrics for ct\n",
    "ct_metrics_results[\"Neural Network\"] = ct_nn_metrics\n",
    "\n",
    "ct_metrics_df = pd.DataFrame(ct_metrics_results).T\n",
    "\n",
    "# keep 3 digits after the decimal point\n",
    "ct_metrics_df = ct_metrics_df.round(3)\n",
    "\n",
    "# export as csv\n",
    "ct_metrics_df.to_csv('./split3_ct_metrics_nn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion-env",
   "language": "python",
   "name": "fusion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
